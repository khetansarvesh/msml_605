4.1. Pre-processing and Vocabulary Decisions
* Explain your tokenization, sentence boundary and other preprocessing  strategies.
4.2. Impact of N-gram Order
* Compare the perplexity results for all models.
* Discuss the trend you observe and explain this phenomenon in terms of the Markov Assumption and Data Sparsity.
4.3. Comparison of Smoothing/Backoff Strategies
* Compare the final perplexity scores of the best models.
* Discussion:
   * Why are the perplexity scores for the unsmoothed models so high/infinite, and how did smoothing correct this?
   * Compare the performance of the backoff/interpolation strategies. Which one performed the best and why do you think this is?


4.4. Qualitative Analysis (Generated Text)
* Implement a function to generate a sequence of words using the probability distribution of your best performing  model.
* Task: Generate at least 5 distinct sentences and include them in your report.
* Discussion: How "fluent" or "human-like" does the generated text appear? Provide a brief explanation of how the model manages to generate these sequences.



add figures / tables / ressults